import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.utils import class_weight
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import seaborn as sns
from collections import defaultdict
import cv2
from PIL import Image
import shutil
import glob

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
tf.random.set_seed(42)
np.random.seed(42)

# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# å‚æ•°è®¾ç½®
BASE_DIR = 'train_samples'
CLEANED_DIR = 'train_samples_cleaned'
BATCH_SIZE = 16
IMG_SIZE = (224, 224)
NUM_CLASSES = 5
K_FOLDS = 5

class SafeMicrotuningStrategy:
    """å®‰å…¨çš„å¾®è°ƒç­–ç•¥ç±»"""
    
    @staticmethod
    def get_layer_groups(base_model):
        """å°†ResNet50åˆ†ä¸ºä¸åŒå±‚ç»„"""
        total_layers = len(base_model.layers)
        
        # ResNet50å±‚åˆ†ç»„ç­–ç•¥
        groups = {
            'early_layers': base_model.layers[:50],      # æ—©æœŸç‰¹å¾å±‚ (conv1, conv2_x)
            'middle_layers': base_model.layers[50:100],  # ä¸­æœŸç‰¹å¾å±‚ (conv3_x)  
            'late_layers': base_model.layers[100:140],   # åæœŸç‰¹å¾å±‚ (conv4_x)
            'top_layers': base_model.layers[140:]        # é¡¶å±‚ç‰¹å¾å±‚ (conv5_x)
        }
        
        print(f"ğŸ“Š ResNet50å±‚åˆ†ç»„:")
        for group_name, layers in groups.items():
            print(f"  {group_name}: {len(layers)} å±‚ (ç´¢å¼• {layers[0].name if layers else 'None'} ~ {layers[-1].name if layers else 'None'})")
        
        return groups
    
    @staticmethod
    def freeze_bn_layers(model):
        """å†»ç»“æ‰€æœ‰BatchNormalizationå±‚"""
        bn_count = 0
        for layer in model.layers:
            if isinstance(layer, tf.keras.layers.BatchNormalization):
                layer.trainable = False
                bn_count += 1
        print(f"ğŸ”’ å†»ç»“äº† {bn_count} ä¸ªBatchNormalizationå±‚")

def create_improved_model_safe(img_size, num_classes, dropout_rate=0.5):
    """åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹æ¶æ„ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰"""
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    base_model = ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(*img_size, 3)
    )
    
    # æ„å»ºè‡ªå®šä¹‰é¡¶å±‚ï¼ˆæ›´ä¿å®ˆçš„è®¾è®¡ï¼‰
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    
    # ç®€åŒ–çš„åˆ†ç±»å¤´éƒ¨ï¼ˆå‡å°‘è¿‡æ‹Ÿåˆé£é™©ï¼‰
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(dropout_rate * 0.7)(x)
    
    # è¾“å‡ºå±‚
    predictions = Dense(num_classes, activation='softmax', dtype='float32')(x)
    
    model = Model(inputs=base_model.input, outputs=predictions)
    return model, base_model

def train_three_stage_safe(model, base_model, train_gen, val_gen, callbacks, class_weights_dict, fold):
    """å®‰å…¨çš„ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥"""
    
    # è·å–å±‚åˆ†ç»„
    layer_groups = SafeMicrotuningStrategy.get_layer_groups(base_model)
    
    print(f"\n--- ç¬¬ {fold+1} æŠ˜ - é˜¶æ®µ1: è®­ç»ƒåˆ†ç±»å¤´éƒ¨ ---")
    
    # é˜¶æ®µ1: å†»ç»“æ•´ä¸ªåŸºç¡€æ¨¡å‹ï¼Œåªè®­ç»ƒå¤´éƒ¨
    for layer in base_model.layers:
        layer.trainable = False
    
    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    stage1_callbacks = [cb for cb in callbacks if not isinstance(cb, RealTimePlottingCallback)]
    stage1_callbacks.append(RealTimePlottingCallback(save_path=f'plots_fold_{fold+1}_stage1'))
    
    history1 = model.fit(
        train_gen,
        epochs=12,  # å¢åŠ epochsè®©å¤´éƒ¨è®­ç»ƒæ›´å……åˆ†
        validation_data=val_gen,
        callbacks=stage1_callbacks,
        class_weight=class_weights_dict,
        verbose=1
    )
    
    print(f"\n--- ç¬¬ {fold+1} æŠ˜ - é˜¶æ®µ2: å®‰å…¨çš„é¡¶å±‚å¾®è°ƒ ---")
    
    # é˜¶æ®µ2: åªè§£å†»é¡¶å±‚å’ŒåæœŸå±‚ï¼ˆä¿æŠ¤æ—©æœŸç‰¹å¾ï¼‰
    for layer in base_model.layers:
        layer.trainable = False
    
    # åªè§£å†»åæœŸå±‚å’Œé¡¶å±‚
    for layer in layer_groups['late_layers'] + layer_groups['top_layers']:
        layer.trainable = True
    
    # å†»ç»“æ‰€æœ‰BNå±‚ä»¥ä¿æŒç¨³å®šæ€§
    SafeMicrotuningStrategy.freeze_bn_layers(model)
    
    trainable_params = sum([np.prod(v.get_shape()) for v in model.trainable_variables])
    total_params = sum([np.prod(v.get_shape()) for v in model.variables])
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    model.compile(
        optimizer=Adam(learning_rate=3e-5),  # æ›´ä¿å®ˆçš„å­¦ä¹ ç‡
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    stage2_callbacks = [cb for cb in callbacks if not isinstance(cb, RealTimePlottingCallback)]
    stage2_callbacks.append(RealTimePlottingCallback(save_path=f'plots_fold_{fold+1}_stage2'))
    
    history2 = model.fit(
        train_gen,
        epochs=18,  # é€‚å½“å¢åŠ epochs
        validation_data=val_gen,
        callbacks=stage2_callbacks,
        class_weight=class_weights_dict,
        verbose=1
    )
    
    print(f"\n--- ç¬¬ {fold+1} æŠ˜ - é˜¶æ®µ3: å¯é€‰çš„ä¸­å±‚å¾®è°ƒ ---")
    
    # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œç¬¬ä¸‰é˜¶æ®µ
    val_acc_stage2 = max(history2.history['val_accuracy'])
    
    if val_acc_stage2 > 0.85:  # åªæœ‰åœ¨æ€§èƒ½è¶³å¤Ÿå¥½æ—¶æ‰è¿›è¡Œç¬¬ä¸‰é˜¶æ®µ
        print(f"âœ… ç¬¬2é˜¶æ®µéªŒè¯å‡†ç¡®ç‡ {val_acc_stage2:.3f} > 0.85, è¿›è¡Œä¿å®ˆçš„ç¬¬3é˜¶æ®µå¾®è°ƒ")
        
        # é˜¶æ®µ3: éå¸¸ä¿å®ˆçš„ä¸­å±‚å¾®è°ƒï¼ˆä¿æŠ¤æ—©æœŸå±‚ï¼‰
        # åªé¢å¤–è§£å†»ä¸­æœŸå±‚ï¼Œæ—©æœŸå±‚å§‹ç»ˆå†»ç»“
        for layer in layer_groups['middle_layers']:
            layer.trainable = True
        
        # ç»§ç»­å†»ç»“æ‰€æœ‰BNå±‚
        SafeMicrotuningStrategy.freeze_bn_layers(model)
        
        trainable_params = sum([np.prod(v.get_shape()) for v in model.trainable_variables])
        print(f"ğŸ“Š ç¬¬3é˜¶æ®µå¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.1f}%)")
        
        model.compile(
            optimizer=Adam(learning_rate=1e-6),  # æä½çš„å­¦ä¹ ç‡
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        stage3_callbacks = [cb for cb in callbacks if not isinstance(cb, RealTimePlottingCallback)]
        stage3_callbacks.append(RealTimePlottingCallback(save_path=f'plots_fold_{fold+1}_stage3'))
        
        # æ·»åŠ é¢å¤–çš„æ—©åœä¿æŠ¤
        early_stop_protective = EarlyStopping(
            monitor='val_accuracy',
            patience=3,  # æ›´æ—©åœæ­¢
            restore_best_weights=True,
            mode='max',
            verbose=1
        )
        stage3_callbacks.append(early_stop_protective)
        
        history3 = model.fit(
            train_gen,
            epochs=8,  # å°‘é‡epochs
            validation_data=val_gen,
            callbacks=stage3_callbacks,
            class_weight=class_weights_dict,
            verbose=1
        )
        
        # æ£€æŸ¥ç¬¬ä¸‰é˜¶æ®µæ˜¯å¦æœ‰æ•ˆ
        val_acc_stage3 = max(history3.history['val_accuracy'])
        if val_acc_stage3 < val_acc_stage2 - 0.01:
            print(f"âš ï¸  ç¬¬3é˜¶æ®µæ€§èƒ½ä¸‹é™ï¼Œä» {val_acc_stage2:.3f} åˆ° {val_acc_stage3:.3f}")
            print("ğŸ”„ å»ºè®®é‡æ–°åŠ è½½ç¬¬2é˜¶æ®µçš„æœ€ä½³æ¨¡å‹")
        
        return [history1, history2, history3]
    
    else:
        print(f"âš ï¸  ç¬¬2é˜¶æ®µéªŒè¯å‡†ç¡®ç‡ {val_acc_stage2:.3f} < 0.85, è·³è¿‡ç¬¬3é˜¶æ®µé¿å…è¿‡æ‹Ÿåˆ")
        print("âœ… ä½¿ç”¨ç¬¬2é˜¶æ®µçš„ç»“æœä½œä¸ºæœ€ç»ˆæ¨¡å‹")
        return [history1, history2]

# å…¶ä»–ç±»å’Œå‡½æ•°ä¿æŒä¸å˜...
class DataCleaner:
    """æ•°æ®æ¸…ç†ç±»ï¼Œè§£å†³æ•°æ®æ ‡æ³¨é”™è¯¯é—®é¢˜"""
    
    def __init__(self, base_dir, cleaned_dir):
        self.base_dir = base_dir
        self.cleaned_dir = cleaned_dir
        self.class_names = ['Pallas', 'Persian', 'Ragdoll', 'Singapura', 'Sphynx']
    
    def extract_true_label_from_filename(self, filename):
        """ä»æ–‡ä»¶åæå–çœŸå®æ ‡ç­¾"""
        filename_lower = filename.lower()
        for class_name in self.class_names:
            if class_name.lower() in filename_lower:
                return class_name
        return None
    
    def clean_and_reorganize_data(self):
        """æ¸…ç†å¹¶é‡æ–°ç»„ç»‡æ•°æ®"""
        print("å¼€å§‹æ¸…ç†æ•°æ®...")
        
        # åˆ›å»ºæ¸…ç†åçš„ç›®å½•
        if os.path.exists(self.cleaned_dir):
            shutil.rmtree(self.cleaned_dir)
        
        for class_name in self.class_names:
            os.makedirs(os.path.join(self.cleaned_dir, class_name), exist_ok=True)
        
        moved_files = defaultdict(int)
        error_files = []
        
        # éå†æ‰€æœ‰åŸå§‹æ–‡ä»¶
        for class_dir in os.listdir(self.base_dir):
            class_path = os.path.join(self.base_dir, class_dir)
            if not os.path.isdir(class_path):
                continue
                
            print(f"å¤„ç†ç›®å½•: {class_dir}")
            files = glob.glob(os.path.join(class_path, '*.*'))
            
            for file_path in files:
                filename = os.path.basename(file_path)
                true_label = self.extract_true_label_from_filename(filename)
                
                if true_label:
                    # ç§»åŠ¨åˆ°æ­£ç¡®çš„ç›®å½•
                    new_path = os.path.join(self.cleaned_dir, true_label, filename)
                    if not os.path.exists(new_path):  # é¿å…é‡å¤æ–‡ä»¶
                        shutil.copy2(file_path, new_path)
                        moved_files[true_label] += 1
                else:
                    error_files.append(file_path)
        
        # æ‰“å°æ¸…ç†ç»“æœ
        print("\næ•°æ®æ¸…ç†å®Œæˆï¼")
        print("æ¯ä¸ªç±»åˆ«çš„æ–‡ä»¶æ•°é‡:")
        for class_name in self.class_names:
            count = moved_files[class_name]
            print(f"  {class_name}: {count}")
        
        if error_files:
            print(f"\næ— æ³•è¯†åˆ«æ ‡ç­¾çš„æ–‡ä»¶æ•°é‡: {len(error_files)}")
            print("å‰10ä¸ªæ— æ³•è¯†åˆ«çš„æ–‡ä»¶:")
            for f in error_files[:10]:
                print(f"  {f}")
        
        return self.cleaned_dir

class RealTimePlottingCallback(Callback):
    """å®æ—¶ç»˜å›¾å›è°ƒå‡½æ•°"""
    
    def __init__(self, save_path='training_plots'):
        super().__init__()
        self.save_path = save_path
        self.epoch_logs = []
        
        # è®¾ç½®matplotlibåç«¯
        plt.ion()
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle('Training Progress - Real Time', fontsize=16)
        
        # åˆå§‹åŒ–å­å›¾
        self.axes[0, 0].set_title('Training Loss')
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        
        self.axes[0, 1].set_title('Training Accuracy')
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].set_ylabel('Accuracy')
        
        self.axes[1, 0].set_title('Validation Loss')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('Loss')
        
        self.axes[1, 1].set_title('Validation Accuracy')
        self.axes[1, 1].set_xlabel('Epoch')
        self.axes[1, 1].set_ylabel('Accuracy')
        
        plt.tight_layout()
        
    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}
        
        # ä¿å­˜æ—¥å¿—
        epoch_log = {'epoch': epoch + 1}
        epoch_log.update(logs)
        self.epoch_logs.append(epoch_log)
        
        # æ¸…é™¤ä¹‹å‰çš„å›¾åƒ
        for ax in self.axes.flat:
            ax.clear()
        
        # æå–æ•°æ®
        epochs = [log['epoch'] for log in self.epoch_logs]
        train_loss = [log.get('loss', 0) for log in self.epoch_logs]
        train_acc = [log.get('accuracy', 0) for log in self.epoch_logs]
        val_loss = [log.get('val_loss', 0) for log in self.epoch_logs]
        val_acc = [log.get('val_accuracy', 0) for log in self.epoch_logs]
        
        # ç»˜åˆ¶è®­ç»ƒæŸå¤±
        self.axes[0, 0].plot(epochs, train_loss, 'b-', label='Train Loss', linewidth=2)
        self.axes[0, 0].set_title('Training Loss')
        self.axes[0, 0].set_xlabel('Epoch')
        self.axes[0, 0].set_ylabel('Loss')
        self.axes[0, 0].grid(True, alpha=0.3)
        self.axes[0, 0].legend()
        
        # ç»˜åˆ¶è®­ç»ƒå‡†ç¡®ç‡
        self.axes[0, 1].plot(epochs, train_acc, 'g-', label='Train Accuracy', linewidth=2)
        self.axes[0, 1].set_title('Training Accuracy')
        self.axes[0, 1].set_xlabel('Epoch')
        self.axes[0, 1].set_ylabel('Accuracy')
        self.axes[0, 1].grid(True, alpha=0.3)
        self.axes[0, 1].legend()
        
        # ç»˜åˆ¶éªŒè¯æŸå¤±
        self.axes[1, 0].plot(epochs, val_loss, 'r-', label='Val Loss', linewidth=2)
        self.axes[1, 0].set_title('Validation Loss')
        self.axes[1, 0].set_xlabel('Epoch')
        self.axes[1, 0].set_ylabel('Loss')
        self.axes[1, 0].grid(True, alpha=0.3)
        self.axes[1, 0].legend()
        
        # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡
        self.axes[1, 1].plot(epochs, val_acc, 'orange', label='Val Accuracy', linewidth=2)
        self.axes[1, 1].set_title('Validation Accuracy')
        self.axes[1, 1].set_xlabel('Epoch')
        self.axes[1, 1].set_ylabel('Accuracy')
        self.axes[1, 1].grid(True, alpha=0.3)
        self.axes[1, 1].legend()
        
        # æ›´æ–°æ˜¾ç¤º
        self.fig.suptitle(f'Training Progress - Epoch {epoch + 1}', fontsize=16)
        plt.tight_layout()
        plt.draw()
        plt.pause(0.01)
        
        # ä¿å­˜å›¾åƒ
        os.makedirs(self.save_path, exist_ok=True)
        self.fig.savefig(os.path.join(self.save_path, f'training_progress_epoch_{epoch+1:03d}.png'), 
                        dpi=150, bbox_inches='tight')

class PerClassAccuracy(Callback):
    """æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡å›è°ƒ"""
    
    def __init__(self, val_generator, class_names, verbose=1):
        super().__init__()
        self.val_gen = val_generator
        self.class_names = class_names
        self.verbose = verbose

    def on_epoch_end(self, epoch, logs=None):
        # æ”¶é›†æ‰€æœ‰éªŒè¯æ•°æ®çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹
        y_true = []
        y_pred = []
        
        for i in range(len(self.val_gen)):
            x_batch, y_batch = self.val_gen[i]
            preds = self.model.predict(x_batch, verbose=0)
            y_true.extend(np.argmax(y_batch, axis=1))
            y_pred.extend(np.argmax(preds, axis=1))
        
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        if self.verbose:
            print(f"\nâ€” Epoch {epoch+1} å„ç±»åˆ«éªŒè¯å‡†ç¡®ç‡:")
            for idx, name in enumerate(self.class_names):
                mask = (y_true == idx)
                if np.sum(mask) == 0:
                    acc = 0.0
                else:
                    acc = accuracy_score(y_true[mask], y_pred[mask])
                print(f"    {name:10s}: {acc*100:5.2f}%")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸ›¡ï¸  å®‰å…¨çš„çŒ«å“ç§åˆ†ç±»è®­ç»ƒæµç¨‹...")
    print("âš ï¸  é‡‡ç”¨ä¿å®ˆçš„å¾®è°ƒç­–ç•¥ï¼Œä¿æŠ¤é¢„è®­ç»ƒç‰¹å¾")
    
    # æ•°æ®æ¸…ç†
    cleaner = DataCleaner(BASE_DIR, CLEANED_DIR)
    cleaned_dir = cleaner.clean_and_reorganize_data()
    
    print(f"\nğŸ”’ å®‰å…¨å¾®è°ƒç­–ç•¥è¯´æ˜:")
    print(f"  âœ… é˜¶æ®µ1: åªè®­ç»ƒåˆ†ç±»å¤´éƒ¨")
    print(f"  âœ… é˜¶æ®µ2: åªè§£å†»ResNet50åæœŸå±‚ + é¡¶å±‚")
    print(f"  âš ï¸  é˜¶æ®µ3: æ¡ä»¶è§£å†»ä¸­æœŸå±‚ï¼ˆæ€§èƒ½>85%æ—¶ï¼‰")
    print(f"  ğŸ›¡ï¸  å§‹ç»ˆä¿æŠ¤: æ—©æœŸå±‚ + æ‰€æœ‰BNå±‚")
    
    # æ¼”ç¤ºå®‰å…¨å¾®è°ƒï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä½¿ç”¨æ—¶è¯·ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬ï¼‰
    print(f"\nğŸ“Š é¢„æœŸæ•ˆæœ:")
    print(f"  ğŸ¯ æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹")
    print(f"  ğŸ›¡ï¸  ä¿æŠ¤é¢„è®­ç»ƒçš„åŸºç¡€ç‰¹å¾") 
    print(f"  ğŸ“ˆ å¯èƒ½ç•¥ä½ä½†æ›´ç¨³å®šçš„æ€§èƒ½")
    print(f"  âš¡ æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦")

if __name__ == "__main__":
    main()