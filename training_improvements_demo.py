#!/usr/bin/env python3
"""
çŒ«å“ç§åˆ†ç±»è®­ç»ƒæ”¹è¿› - æ¦‚å¿µæ¼”ç¤º
å±•ç¤ºæ‰€æœ‰æ”¹è¿›æªæ–½çš„æ ¸å¿ƒæ€è·¯å’Œå®ç°é€»è¾‘
"""

import os
import glob
import shutil
import random
from collections import defaultdict, Counter

class DataCleaner:
    """æ•°æ®æ¸…ç†æ¼”ç¤ºç±»"""
    
    def __init__(self, base_dir, cleaned_dir):
        self.base_dir = base_dir
        self.cleaned_dir = cleaned_dir
        self.class_names = ['Pallas', 'Persian', 'Ragdoll', 'Singapura', 'Sphynx']
    
    def extract_true_label_from_filename(self, filename):
        """ä»æ–‡ä»¶åæå–çœŸå®æ ‡ç­¾"""
        filename_lower = filename.lower()
        for class_name in self.class_names:
            if class_name.lower() in filename_lower:
                return class_name
        return None
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡é—®é¢˜"""
        print("ğŸ” åˆ†ææ•°æ®è´¨é‡...")
        print("="*50)
        
        issues_found = defaultdict(list)
        
        for class_dir in os.listdir(self.base_dir):
            class_path = os.path.join(self.base_dir, class_dir)
            if not os.path.isdir(class_path):
                continue
                
            print(f"\nğŸ“ æ£€æŸ¥ç›®å½•: {class_dir}")
            files = glob.glob(os.path.join(class_path, '*.*'))
            
            file_label_counts = Counter()
            
            for file_path in files[:10]:  # åªæ£€æŸ¥å‰10ä¸ªæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
                filename = os.path.basename(file_path)
                true_label = self.extract_true_label_from_filename(filename)
                
                if true_label:
                    file_label_counts[true_label] += 1
                    if true_label != class_dir:
                        issues_found[class_dir].append((filename, true_label))
                else:
                    issues_found[class_dir].append((filename, "æ— æ³•è¯†åˆ«"))
            
            print(f"  ğŸ“Š æ–‡ä»¶æ ‡ç­¾åˆ†å¸ƒ: {dict(file_label_counts)}")
            
            if issues_found[class_dir]:
                print(f"  âš ï¸  å‘ç° {len(issues_found[class_dir])} ä¸ªæ ‡æ³¨é—®é¢˜")
                for filename, detected_label in issues_found[class_dir][:3]:
                    print(f"    - {filename} â†’ åº”è¯¥æ˜¯ {detected_label}")
        
        return issues_found
    
    def clean_and_reorganize_data(self):
        """æ¸…ç†å¹¶é‡æ–°ç»„ç»‡æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        print("\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")
        print("="*50)
        
        # æ¨¡æ‹Ÿæ¸…ç†è¿‡ç¨‹
        moved_files = defaultdict(int)
        
        for class_dir in os.listdir(self.base_dir):
            class_path = os.path.join(self.base_dir, class_dir)
            if not os.path.isdir(class_path):
                continue
                
            files = glob.glob(os.path.join(class_path, '*.*'))
            print(f"ğŸ“ å¤„ç† {class_dir}: {len(files)} ä¸ªæ–‡ä»¶")
            
            for file_path in files:
                filename = os.path.basename(file_path)
                true_label = self.extract_true_label_from_filename(filename)
                
                if true_label:
                    moved_files[true_label] += 1
                    if true_label != class_dir:
                        print(f"  ğŸ“¦ ç§»åŠ¨: {filename} â†’ {true_label}/")
        
        print("\nâœ… æ¸…ç†å®Œæˆï¼")
        print("æ¸…ç†åçš„æ•°æ®åˆ†å¸ƒ:")
        for class_name in self.class_names:
            count = moved_files[class_name]
            print(f"  {class_name}: {count} ä¸ªæ–‡ä»¶")
        
        return moved_files

class KFoldCrossValidation:
    """KæŠ˜äº¤å‰éªŒè¯æ¼”ç¤º"""
    
    def __init__(self, data_distribution, k_folds=5):
        self.data_distribution = data_distribution
        self.k_folds = k_folds
        self.total_samples = sum(data_distribution.values())
    
    def create_stratified_folds(self):
        """åˆ›å»ºåˆ†å±‚KæŠ˜"""
        print(f"\nğŸ”€ åˆ›å»º {self.k_folds} æŠ˜åˆ†å±‚äº¤å‰éªŒè¯...")
        print("="*50)
        
        folds = []
        for fold in range(self.k_folds):
            fold_data = {}
            val_size = self.total_samples // self.k_folds
            
            # æ¨¡æ‹Ÿæ¯ä¸€æŠ˜çš„æ•°æ®åˆ†å¸ƒ
            for class_name, count in self.data_distribution.items():
                val_count = count // self.k_folds
                train_count = count - val_count
                fold_data[class_name] = {
                    'train': train_count,
                    'val': val_count
                }
            
            folds.append(fold_data)
            
            print(f"ğŸ“Š ç¬¬ {fold+1} æŠ˜:")
            for class_name, counts in fold_data.items():
                print(f"  {class_name}: è®­ç»ƒ={counts['train']}, éªŒè¯={counts['val']}")
        
        return folds
    
    def simulate_training(self, folds):
        """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹"""
        print(f"\nğŸš€ å¼€å§‹ {self.k_folds} æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ...")
        print("="*60)
        
        fold_results = []
        
        for fold_idx, fold_data in enumerate(folds):
            print(f"\n--- ç¬¬ {fold_idx+1}/{self.k_folds} æŠ˜è®­ç»ƒ ---")
            
            # æ¨¡æ‹Ÿä¸‰é˜¶æ®µè®­ç»ƒ
            stages = [
                ("é˜¶æ®µ1: å¤´éƒ¨è®­ç»ƒ", 10, 0.85),
                ("é˜¶æ®µ2: é¡¶å±‚å¾®è°ƒ", 15, 0.88), 
                ("é˜¶æ®µ3: å…¨å±€å¾®è°ƒ", 10, 0.91)
            ]
            
            final_acc = 0
            for stage_name, epochs, target_acc in stages:
                print(f"  {stage_name}")
                
                # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
                for epoch in range(1, epochs + 1):
                    # æ¨¡æ‹Ÿå‡†ç¡®ç‡æå‡
                    current_acc = target_acc * (1 - 0.3 * (epochs - epoch) / epochs)
                    current_acc += random.uniform(-0.02, 0.02)  # æ·»åŠ éšæœºå™ªå£°
                    
                    if epoch % 5 == 0 or epoch == epochs:
                        print(f"    Epoch {epoch:2d}: val_acc = {current_acc:.3f}")
                
                final_acc = current_acc
            
            fold_results.append(final_acc)
            print(f"  âœ… ç¬¬ {fold_idx+1} æŠ˜æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_acc:.3f}")
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        mean_acc = sum(fold_results) / len(fold_results)
        std_acc = (sum((x - mean_acc)**2 for x in fold_results) / len(fold_results))**0.5
        
        print(f"\nğŸ“ˆ KæŠ˜äº¤å‰éªŒè¯ç»“æœæ±‡æ€»:")
        print("="*40)
        for i, acc in enumerate(fold_results):
            print(f"ç¬¬ {i+1} æŠ˜: {acc:.3f}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {mean_acc:.3f} Â± {std_acc:.3f}")
        
        return fold_results, mean_acc, std_acc

class RealTimePlottingDemo:
    """å®æ—¶ç»˜å›¾æ¼”ç¤º"""
    
    def __init__(self):
        self.epoch_logs = []
    
    def simulate_training_visualization(self, epochs=25):
        """æ¨¡æ‹Ÿè®­ç»ƒå¯è§†åŒ–"""
        print(f"\nğŸ“Š æ¨¡æ‹Ÿå®æ—¶è®­ç»ƒå¯è§†åŒ– ({epochs} epochs)...")
        print("="*50)
        
        print("ğŸ–¼ï¸  è®­ç»ƒå›¾è¡¨å°†åŒ…å«:")
        print("  ğŸ“ˆ è®­ç»ƒæŸå¤± (Training Loss)")
        print("  ğŸ“ˆ è®­ç»ƒå‡†ç¡®ç‡ (Training Accuracy)")  
        print("  ğŸ“ˆ éªŒè¯æŸå¤± (Validation Loss)")
        print("  ğŸ“ˆ éªŒè¯å‡†ç¡®ç‡ (Validation Accuracy)")
        
        print(f"\nğŸ¯ æ¨¡æ‹Ÿ {epochs} ä¸ªepochsçš„è®­ç»ƒè¿‡ç¨‹:")
        
        for epoch in range(1, epochs + 1):
            # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
            train_loss = 2.0 * (0.9 ** epoch) + random.uniform(-0.1, 0.1)
            train_acc = 1.0 - train_loss / 3.0 + random.uniform(-0.05, 0.05)
            val_loss = train_loss * 1.1 + random.uniform(-0.05, 0.05)
            val_acc = train_acc * 0.95 + random.uniform(-0.03, 0.03)
            
            # é™åˆ¶èŒƒå›´
            train_acc = max(0, min(1, train_acc))
            val_acc = max(0, min(1, val_acc))
            train_loss = max(0, train_loss)
            val_loss = max(0, val_loss)
            
            epoch_log = {
                'epoch': epoch,
                'loss': train_loss,
                'accuracy': train_acc,
                'val_loss': val_loss,
                'val_accuracy': val_acc
            }
            self.epoch_logs.append(epoch_log)
            
            if epoch % 5 == 0 or epoch <= 3:
                print(f"  Epoch {epoch:2d}: "
                      f"loss={train_loss:.3f}, "
                      f"acc={train_acc:.3f}, "
                      f"val_loss={val_loss:.3f}, "
                      f"val_acc={val_acc:.3f}")
                print(f"    ğŸ“¸ ä¿å­˜å›¾è¡¨: training_progress_epoch_{epoch:03d}.png")
        
        print(f"\nâœ… è®­ç»ƒå¯è§†åŒ–å®Œæˆï¼å…±ç”Ÿæˆ {epochs} å¼ è®­ç»ƒå›¾è¡¨")
        return self.epoch_logs

class AntiOverfittingDemo:
    """é˜²è¿‡æ‹Ÿåˆæªæ–½æ¼”ç¤º"""
    
    def demonstrate_techniques(self):
        """å±•ç¤ºé˜²è¿‡æ‹ŸåˆæŠ€æœ¯"""
        print("\nğŸ›¡ï¸  é˜²è¿‡æ‹Ÿåˆæªæ–½è¯¦è§£...")
        print("="*50)
        
        techniques = [
            {
                "name": "æ•°æ®å¢å¼º (Data Augmentation)",
                "description": "å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›",
                "params": {
                    "rotation_range": "20Â°",
                    "width_shift_range": "0.1",
                    "height_shift_range": "0.1", 
                    "zoom_range": "0.1",
                    "brightness_range": "(0.8, 1.2)"
                }
            },
            {
                "name": "Dropoutæ­£åˆ™åŒ–",
                "description": "éšæœºå…³é—­ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡åº¦ä¾èµ–ç‰¹å®šç‰¹å¾",
                "params": {
                    "layer_1_dropout": "0.5",
                    "layer_2_dropout": "0.35",
                    "layer_3_dropout": "0.25"
                }
            },
            {
                "name": "æ‰¹æ ‡å‡†åŒ– (Batch Normalization)",
                "description": "ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼ŒåŠ é€Ÿæ”¶æ•›",
                "params": {
                    "position": "æ¯ä¸ªDenseå±‚ä¹‹å",
                    "momentum": "0.99",
                    "epsilon": "0.001"
                }
            },
            {
                "name": "æ—©åœç­–ç•¥ (Early Stopping)",
                "description": "ç›‘æ§éªŒè¯æŸå¤±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ",
                "params": {
                    "monitor": "val_loss",
                    "patience": "12",
                    "restore_best_weights": "True"
                }
            },
            {
                "name": "å­¦ä¹ ç‡è°ƒåº¦",
                "description": "åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§",
                "params": {
                    "strategy": "ReduceLROnPlateau",
                    "factor": "0.5",
                    "patience": "5"
                }
            },
            {
                "name": "ç±»åˆ«æƒé‡å¹³è¡¡",
                "description": "å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜",
                "params": {
                    "method": "sklearn.class_weight.balanced",
                    "automatic": "True"
                }
            }
        ]
        
        for i, technique in enumerate(techniques, 1):
            print(f"\n{i}. ğŸ”§ {technique['name']}")
            print(f"   ğŸ“ {technique['description']}")
            print(f"   âš™ï¸  å‚æ•°é…ç½®:")
            for param, value in technique['params'].items():
                print(f"      â€¢ {param}: {value}")
        
        print(f"\nğŸ¯ ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥:")
        stages = [
            ("é˜¶æ®µ1", "å¤´éƒ¨è®­ç»ƒ", "å†»ç»“ResNet50åŸºç¡€å±‚ï¼Œåªè®­ç»ƒåˆ†ç±»å¤´", "10 epochs, lr=1e-3"),
            ("é˜¶æ®µ2", "é¡¶å±‚å¾®è°ƒ", "è§£å†»ResNet50é¡¶å±‚15å±‚è¿›è¡Œå¾®è°ƒ", "15 epochs, lr=5e-5"),
            ("é˜¶æ®µ3", "å…¨å±€å¾®è°ƒ", "è§£å†»æ‰€æœ‰å±‚ï¼Œæä½å­¦ä¹ ç‡å…¨å±€ä¼˜åŒ–", "10 epochs, lr=1e-6")
        ]
        
        for stage, name, desc, params in stages:
            print(f"  ğŸ“ {stage} - {name}")
            print(f"    ğŸ“‹ {desc}")
            print(f"    âš™ï¸  {params}")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ± çŒ«å“ç§åˆ†ç±»è®­ç»ƒæ”¹è¿› - å®Œæ•´æ¼”ç¤º")
    print("="*60)
    print("æœ¬æ¼”ç¤ºå±•ç¤ºäº†æ‰€æœ‰è®­ç»ƒæ”¹è¿›æªæ–½çš„æ ¸å¿ƒæ€è·¯å’Œå®ç°é€»è¾‘")
    print()
    
    # 1. æ•°æ®è´¨é‡åˆ†æå’Œæ¸…ç†
    print("ğŸ“‚ 1. æ•°æ®è´¨é‡åˆ†æä¸æ¸…ç†")
    cleaner = DataCleaner('train_samples', 'train_samples_cleaned')
    
    # åˆ†ææ•°æ®è´¨é‡é—®é¢˜
    issues = cleaner.analyze_data_quality()
    
    # æ¨¡æ‹Ÿæ•°æ®æ¸…ç†
    cleaned_distribution = cleaner.clean_and_reorganize_data()
    
    # 2. KæŠ˜äº¤å‰éªŒè¯
    print("\nğŸ”€ 2. KæŠ˜äº¤å‰éªŒè¯")
    kfold = KFoldCrossValidation(cleaned_distribution, k_folds=5)
    folds = kfold.create_stratified_folds()
    fold_results, mean_acc, std_acc = kfold.simulate_training(folds)
    
    # 3. å®æ—¶è®­ç»ƒå¯è§†åŒ–
    print("\nğŸ“Š 3. å®æ—¶è®­ç»ƒå¯è§†åŒ–")
    plotter = RealTimePlottingDemo()
    training_logs = plotter.simulate_training_visualization(epochs=25)
    
    # 4. é˜²è¿‡æ‹Ÿåˆæªæ–½
    print("\nğŸ›¡ï¸  4. é˜²è¿‡æ‹Ÿåˆæªæ–½")
    anti_overfit = AntiOverfittingDemo()
    anti_overfit.demonstrate_techniques()
    
    # 5. æ€»ç»“æ”¹è¿›æ•ˆæœ
    print(f"\nğŸ“ˆ 5. é¢„æœŸæ”¹è¿›æ•ˆæœæ€»ç»“")
    print("="*50)
    print(f"âœ… æ•°æ®è´¨é‡: ä¿®å¤æ ‡æ³¨é”™è¯¯ï¼Œæé«˜æ•°æ®ä¸€è‡´æ€§")
    print(f"âœ… æ¨¡å‹ç¨³å®šæ€§: KæŠ˜CVå¹³å‡å‡†ç¡®ç‡ {mean_acc:.3f} Â± {std_acc:.3f}")
    print(f"âœ… è®­ç»ƒå¯è§‚æµ‹æ€§: ç”Ÿæˆ {len(training_logs)} ä¸ªepochçš„å®æ—¶å›¾è¡¨")
    print(f"âœ… è¿‡æ‹Ÿåˆé˜²æŠ¤: 6ç§é˜²è¿‡æ‹ŸåˆæŠ€æœ¯ç»¼åˆåº”ç”¨")
    print(f"âœ… é¢„æœŸæµ‹è¯•å‡†ç¡®ç‡: ä»53.6%æå‡è‡³>80%")
    
    print(f"\nğŸ¯ ä½¿ç”¨å»ºè®®:")
    print(f"1. å®‰è£…ä¾èµ–: pip install -r requirements.txt")
    print(f"2. è¿è¡Œè®­ç»ƒ: python improved_training.py")  
    print(f"3. è¯„ä¼°æ¨¡å‹: python improved_testing.py")
    print(f"4. æŸ¥çœ‹æ–‡æ¡£: cat TRAINING_IMPROVEMENTS.md")
    
    print(f"\nğŸš€ è®­ç»ƒæ”¹è¿›æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main()